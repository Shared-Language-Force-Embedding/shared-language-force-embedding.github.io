---
layout: page
title: Results
permalink: /results/
---

<h2>Metrics</h2>
    
<ul>
    <li><strong>Force Profile Accuracy (FPAcc):</strong> Measures how closely the predicted force profile \( \hat{x}_f \) matches the ground truth \( x_f \) using Mean Squared Error (MSE).  
        $$
        \text{FPAcc} = \text{MSE}(\hat{x}_f, x_f)
        $$
    </li>
    <li><strong>Force Direction Accuracy (FDAcc):</strong> Evaluates directional alignment between predicted impulse \( \hat{J}(T) \) and ground truth impulse \( J(T) \) using cosine similarity.  
        $$
        \text{FDAcc} = \frac{\hat{J}(T) \cdot J(T)}{||\hat{J}(T)|| \cdot ||J(T)||}
        $$
    </li>
    <li><strong>Modifier Similarity (ModSim):</strong> Cosine similarity between the predicted modifier \( \hat{w}_m \) and the ground truth \( w_m \) using SBERT embeddings \( E \).  
        $$
        \text{ModSim} = \frac{E(\hat{w}_m) \cdot E(w_m)}{||E(\hat{w}_m)|| \cdot ||E(w_m)||}
        $$
    </li>
    <li><strong>Direction Similarity (DirSim):</strong> Cosine similarity between predicted direction words \( \hat{w}_d \) and ground truth \( w_d \).  
        $$
        \text{DirSim} = \frac{E(\hat{w}_d) \cdot E(w_d)}{||E(\hat{w}_d)|| \cdot ||E(w_d)||}
        $$
    </li>
    <li><strong>Full Phrase Similarity (PhraseSim):</strong> Average of \( \text{ModSim} \) and \( \text{DirSim} \).  
        $$
        \text{PhraseSim} = \frac{1}{2} \left(\text{ModSim} + \text{DirSim} \right)
        $$
    </li>
</ul>

<h2>Baseline Models</h2>
<ul>
    <li>$\text{SVM/KNN}$: Uses SVM to classify force signals into phrases and KNN to predict force profiles from phrases.</li>
    <li>$\text{DMLP}_B$: Direct MLP mapping between force profiles and binary phrase vectors without a shared latent space.</li>
    <li>$\text{DMLP}_S$: Direct MLP mapping between force profiles and SBERT-embedded phrases without a shared latent space.</li>
    <li>$\text{DAE}_B$ (Ours): Dual autoencoder with binary phrase vectors.</li>
    <li>$\text{DAE}_S$ (Ours): Dual autoencoder with SBET embeddings.</li>
</ul>

<h2>Experiments</h2>
<p>To evaluate the effectiveness of our cross-modality embedding framework, we conducted three main experiments:</p>
<ol>
    <li><strong>In-Distribution Evaluation:</strong> The dataset was randomly split into 90% training and 10% testing sets. Models were trained and tested on the same distribution of force–phrase pairs and averaged across 30 independent seeds to reduce variability in the results.</li>
    
    <li><strong>Out-of-Distribution Modifiers:</strong> To test generalization, we systematically held out one modifier word (e.g., “slowly”) from the training set while keeping it in the test set. This experiment measured how well the models handled unseen adverbial variations in force descriptions.</li>
    
    <li><strong>Out-of-Distribution Directions:</strong> Similar to the modifier experiment, we excluded a single direction word (e.g., “up”) from training and evaluated the model’s ability to generalize to new directional inputs. This tested whether the models could infer force trajectories for previously unseen movement directions.</li>
</ol>


<h2>Overall Results</h2>
<p>Our framework outperforms baseline models in both force-to-language and language-to-force translation, demonstrating the effectiveness of a shared latent space.</p>

{% include overall_results.html %}

<h2>Analysis</h2>
<p>Our results showed that the dual autoencoder (DAE) models significantly outperformed baseline approaches in both force-to-language and language-to-force translation. The DAE models achieved better performance than simpler SVM and MLP-based baselines, demonstrating that a shared latent space effectively captures the bidirectional relationship between physical force and linguistic descriptions.</p>

<p>Additionally, the models successfully generalized to unseen modifiers and directions, particularly when using SBERT embeddings, which improved force profile reconstruction. However, we observed a trade-off between force reconstruction accuracy and linguistic precision: the SBERT-based model (\( \text{DAE}_S \)) was better at estimating realistic force trajectories, while the binary phrase vector model (\( \text{DAE}_S \)) provided more precise textual descriptions.</p>

<p>These findings confirm that integrating force and language into a unified representation enhances natural human-robot communication, with different representation choices offering advantages depending on whether force accuracy or linguistic fidelity is prioritized.</p>